# -*- coding: utf-8 -*-
"""CV_fyp_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10SxZiCN9R4dDyC3GzTq2S1oHztWRgv9H
"""

from google.colab import drive
drive.mount('/content/drive')

import keras
import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
from tqdm import tqdm

DATADIR = "/content/drive/My Drive/eye_resize(224,224)/"
CATEGORIES = ["central", "hemianopia","normal","quadratopia","superior","turnel"]

for category in CATEGORIES:  # do dogs and cats
    path = os.path.join(DATADIR,category)  # create path to dogs and cats
    for img in os.listdir(path):  # iterate over each image per dogs and cats
        img_array = cv2.imread(os.path.join(path,img) )  # convert to array ,cv2.IMREAD_GRAYSCALE
        #plt.imshow(img_array, cmap='gray')  # graph it
        #plt.show()  # display!

        break  # we just want one for now so break
    break 

training_data = []

def create_training_data():
    for category in CATEGORIES:  # do dogs and cats

        path = os.path.join(DATADIR,category)  # create path to dogs and cats
        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat

        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats
            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)  # convert to array ,cv2.IMREAD_GRAYSCALE
            new_array = cv2.resize(img_array, (224, 224))  # resize to normalize data size
            training_data.append([new_array, class_num])  # add this to our training_data

create_training_data()

print(len(training_data))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import KFold
from tensorflow import keras as K
import tensorflow.keras.layers as L

X = []
y = []

for features,label in training_data:
    X.append(features)
    y.append(label)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, stratify=y,random_state=42)

x_train = np.asarray(X_train,dtype=np.float32)/255.0
  #print("image shape:",x_train[0].shape)
y_train = keras.utils.to_categorical(y_train, num_classes=6, dtype='float32')
x_test = np.asarray(X_test,dtype=np.float32)/255.0
  #print("image shape:",x_test[0].shape)
y_test2 = keras.utils.to_categorical(y_test, num_classes=6, dtype='float32')
  #add dimentional
x_train = x_train[...,np.newaxis]
x_test2 = x_test[...,np.newaxis]

def cnn_model():
  """
  model = Sequential()
  model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(32,32,1)))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(32, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Dropout(0.25))
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(6, activation='softmax'))

  model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
  """
    input = L.Input(shape=(224,224,1))

  block1 = L.BatchNormalization(name='norm_0')(input)

  # Block 1
  block1 = L.Conv2D(16, (3,3), name='conv_11', activation='relu')(block1)
  block1 = L.Conv2D(32, (3,3), name='conv_12', activation='relu')(block1)
  block1 = L.Conv2D(64, (3,3), name='conv_13', activation='relu')(block1)
  block1 = L.Conv2D(128, (3,3), name='conv_14', activation='relu')(block1)
  block1 = L.MaxPooling2D(pool_size=(2, 2))(block1)
  block1 = L.BatchNormalization(name='norm_1')(block1)

  block1 = L.Conv2D(32, 1)(block1)

  # Block 2
  block2 = L.Conv2D(64, (3,3), name='conv_21', activation='relu')(block1)
  block2 = L.Conv2D(128, (3,3), name='conv_22', activation='relu')(block2)
  block2 = L.Conv2D(128, (3,3), name='conv_23', activation='relu')(block2)
  block2 = L.Conv2D(152, (3,3), name='conv_24', activation='relu')(block2)
  block2 = L.MaxPooling2D(pool_size=(2, 2))(block2)
  block2 = L.BatchNormalization(name='norm_2')(block2)

  block2 = L.Conv2D(128, 1)(block2)

  # Block 3
  block3 = L.Conv2D(128, (3,3), name='conv_31', activation='relu')(block2)
  block3 = L.Conv2D(512, (3,3), name='conv_32', activation='relu')(block3)
  block3 = L.Conv2D(512, (3,3), name='conv_33', activation='relu')(block3)
  block3 = L.Dropout(0.2)(block3)
  block3 = L.Conv2D(128, (3,3), name='conv_34', activation='relu')(block3)
  #block3 = L.Dropout(0.2)(block3)
  block3 = L.MaxPooling2D(pool_size=(2, 2))(block3)
  block3 = L.BatchNormalization(name='norm_3')(block3)

  # Block 4
  block4 = L.Conv2D(128, (3,3), name='conv_41', activation='relu')(block3)
  block4 = L.Conv2D(64, (3,3), name='conv_42', activation='relu')(block4)
  block4 = L.Conv2D(32, (3,3), name='conv_43', activation='relu')(block4)
  block4 = L.Conv2D(16, (2,2), name='conv_44', activation='relu')(block4)
  block4 = L.MaxPooling2D(pool_size=(2, 2))(block4)
  block4 = L.BatchNormalization(name='norm_4')(block4)

  block4 = L.Conv2D(8, 1)(block4)

  block5 = L.GlobalAveragePooling2D()(block4)
  #output = L.Activation('softmax')(block5)
  output = L.Dense(6,activation="softmax")(block5)

  model = K.Model(inputs=[input], outputs=[output])
  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.0001) , metrics=["accuracy"])
  return model
model = cnn_model()
model2 = KerasClassifier(build_fn=cnn_model, epochs=100, batch_size=32, verbose=1)

kfold = KFold(n_splits=5, shuffle=True, random_state=42) #training and validation
acc = cross_val_score(model2, x_train, y_train, cv=kfold)

history = model.fit(x_train,y_train,validation_data=(x_test2,y_test2),batch_size=32,epochs=100)
test_eval = model.evaluate(x_test2, y_test2, verbose=0)
print(test_eval)

plt.plot(history.history['loss'], label = "Train") 
plt.plot(history.history['val_loss'], label = "Validation")
plt.title("losses")
plt.xlabel("epochs")
plt.ylabel("loss")
plt.show()
plt.plot(history.history['accuracy'], label = "Train")
plt.plot(history.history['val_accuracy'], label = "Validation") 
plt.title("Model Accuracy")
plt.xlabel("epochs")
plt.ylabel("Accuracy[%]")
plt.legend()
plt.show()

print('accuracy=',((acc[0]+acc[1]+acc[2]+acc[3]+acc[4])/5))

from sklearn.metrics import classification_report
predictions = model.predict(x_test2)
predictions[0]
print(len(predictions))


pred=[]
for i in range(235):
	predictions[i]
	A=np.argmax(predictions[i])
	pred.append(A)
print(classification_report(pred,y_test,target_names=CATEGORIES))